{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzdeYuzmH7-G"
      },
      "source": [
        "Yohan Pesuna(I036).\n",
        "\n",
        "\n",
        "Image Caption Generator\n",
        "\n",
        "\n",
        "VGG 16 architecture - Convolutional Neural Network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWBakvm4EN3B"
      },
      "source": [
        "import string\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "\n",
        "from os import listdir\n",
        "from pickle import dump, load\n",
        "\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers.merge import add\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, Dense, LSTM, Embedding, Dropout\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from nltk.translate.bleu_score import corpus_bleu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAHDwSBffI6s",
        "outputId": "1fb99c4b-3245-464d-9d22-684e43e33a61"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XquWx0yeETz3"
      },
      "source": [
        "# Extract features from each photo in the directory\n",
        "def extract_features(directory):\n",
        "    \n",
        "    # Loading the model\n",
        "    model = VGG16()\n",
        "\n",
        "    # Removing the last layer from the loaded model as we require only the features not the classification \n",
        "    model.layers.pop()\n",
        "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
        "    \n",
        "    # Summarizing the model \n",
        "    print(model.summary())\n",
        "\n",
        "    # Extracting features from each photo and storing it in a dictionary \n",
        "    features = dict()\n",
        "\n",
        "    for name in listdir(directory):\n",
        "\n",
        "        # Defining the path of the image \n",
        "        filename = directory + '/' + name\n",
        "        \n",
        "        # Loading an image and converting it into size 224 * 224\n",
        "        image = load_img(filename, target_size=(224, 224))\n",
        "        \n",
        "        # Converting the image pixels into a numpy array\n",
        "        image = img_to_array(image)\n",
        "        \n",
        "        # Reshaping data for the model\n",
        "        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "\n",
        "        \n",
        "        # Preprocessing the images for the VGG model\n",
        "        # The preprocess_input function is meant to adequate your image to the format the model requires.\n",
        "        image = preprocess_input(image)\n",
        "\n",
        "        # Getting features of an image\n",
        "        feature = model.predict(image, verbose=0)\n",
        "        \n",
        "        # Getting the image name\n",
        "        image_id = name.split('.')[0]\n",
        "\n",
        "        # Storing the feature corresponding to the image in the dictionary\n",
        "        features[image_id] = feature\n",
        "        \n",
        "        # print('>%s' % name)\n",
        "        \n",
        "    return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GHSLWb6FIMA",
        "outputId": "a1b09bb8-380c-48d3-ef4b-0168b676651e"
      },
      "source": [
        "# Defining the directory we are using\n",
        "directory = 'drive/MyDrive/Flicker8k_Dataset/'\n",
        "\n",
        "# Extracting features from all the images\n",
        "features = extract_features(directory)\n",
        "\n",
        "print('Extracted Features: ', len(features))\n",
        "\n",
        "# Dumping the features in a pickle file for further use\n",
        "dump(features, open('features.pkl', 'wb'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "predictions (Dense)          (None, 1000)              4097000   \n",
            "=================================================================\n",
            "Total params: 138,357,544\n",
            "Trainable params: 138,357,544\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Extracted Features:  8091\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzjjhUGEFMV9"
      },
      "source": [
        "# Loading the file containg all the descriptions into memory\n",
        "\n",
        "def load_doc(filename):\n",
        "    # Opening the file as read only\n",
        "    file = open(filename, 'r')\n",
        "\n",
        "    # Reading all text and storing it.\n",
        "    text = file.read()\n",
        "\n",
        "    # Closing the file\n",
        "    file.close()\n",
        "    \n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OI-lXrzdFRUO"
      },
      "source": [
        "def photo_to_description_mapping(descriptions):\n",
        "    \n",
        "    # Dictionary to store the mapping of photo identifiers to descriptions\n",
        "    description_mapping = dict()\n",
        "    \n",
        "    # Iterating through each line of the descriptions\n",
        "    for line in descriptions.split('\\n'):\n",
        "        \n",
        "        # Splitting the lines by white space\n",
        "        words = line.split()\n",
        "        \n",
        "        # Skipping the lines with length less than 2\n",
        "        if len(line)<2:\n",
        "            continue\n",
        "            \n",
        "        # The first word is the image_id and the rest are the part of the description of that image\n",
        "        image_id, image_description = words[0], words[1:]\n",
        "        \n",
        "        # Retaining only the name of the image and removing the extension from it\n",
        "        image_id = image_id.split('.')[0]\n",
        "        \n",
        "        # Image_descriptions contains comma separated words of the description, hence, converting it back to string\n",
        "        image_description = ' '.join(image_description)\n",
        "        \n",
        "        # There are multiple descriptions per image, \n",
        "        # hence, corresponding to every image identifier in the dictionary, there is a list of description\n",
        "        # if the list does not exist then we need to create it\n",
        "        \n",
        "        if image_id not in description_mapping:\n",
        "            description_mapping[image_id] = list()\n",
        "            \n",
        "        # Now storing the descriptions in the mapping\n",
        "        description_mapping[image_id].append(image_description)\n",
        "    \n",
        "    return description_mapping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsGLMPKCFVQY"
      },
      "source": [
        "def clean_descriptions(description_mapping):\n",
        "    \n",
        "    # Preapring a translation table for removing all the punctuation\n",
        "    table = str.maketrans('','', string.punctuation)\n",
        "    \n",
        "    # Traversing through the mapping we created\n",
        "    for key, descriptions in description_mapping.items():\n",
        "        for i in range(len(descriptions)):\n",
        "            description = descriptions[i]\n",
        "            description = description.split()\n",
        "            \n",
        "            # Converting all the words to lower case\n",
        "            description = [word.lower() for word in description]\n",
        "            \n",
        "            # Removing the punctuation using the translation table we made\n",
        "            description = [word.translate(table) for word in description]\n",
        "            \n",
        "            # Removing the words with length =1\n",
        "            description = [word for word in description if len(word)>1]\n",
        "            \n",
        "            # Removing all words with number in them\n",
        "            description = [word for word in description if word.isalpha()]\n",
        "            \n",
        "            # Converting the description back to string and overwriting in the descriptions list\n",
        "            descriptions[i] = ' '.join(description)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8safUzWFXY_"
      },
      "source": [
        "# Converting the loaded descriptions into a vocabulary of words\n",
        "\n",
        "def to_vocabulary(descriptions):\n",
        "    \n",
        "    # Build a list of all description strings\n",
        "    all_desc = set()\n",
        "    \n",
        "    for key in descriptions.keys():\n",
        "        [all_desc.update(d.split()) for d in descriptions[key]]\n",
        "    \n",
        "    return all_desc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcXs6d7TFXva"
      },
      "source": [
        "# save descriptions to file, one per line\n",
        "def save_descriptions(descriptions, filename):\n",
        "    lines = list()\n",
        "    for key, desc_list in descriptions.items():\n",
        "        for desc in desc_list:\n",
        "            lines.append(key + ' ' + desc)\n",
        "    data = '\\n'.join(lines)\n",
        "    file = open(filename, 'w')\n",
        "    file.write(data)\n",
        "    file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UawRIqBxFc2w",
        "outputId": "bc7b2a3f-9be5-46a0-d9b9-715e07c91d46"
      },
      "source": [
        "filename = 'Flickr8k.token.txt'\n",
        "\n",
        "# Loading descriptions\n",
        "doc = load_doc(filename)\n",
        "\n",
        "# Parsing descriptions\n",
        "descriptions = photo_to_description_mapping(doc)\n",
        "print('Loaded: %d ' % len(descriptions))\n",
        "\n",
        "# Cleaning the descriptions\n",
        "clean_descriptions(descriptions)\n",
        "\n",
        "# Summarizing the vocabulary\n",
        "vocabulary = to_vocabulary(descriptions)\n",
        "print('Vocabulary Size: %d' % len(vocabulary))\n",
        "\n",
        "# Saving to the file\n",
        "save_descriptions(descriptions, 'descriptions.txt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded: 8092 \n",
            "Vocabulary Size: 8763\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoF21G1vFh6F"
      },
      "source": [
        "# Function for loading a file into memory and returning text from it\n",
        "def load_file(filename):\n",
        "    file = open(filename, 'r')\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "# Function for loading a pre-defined list of photo identifiers\n",
        "def load_photo_identifiers(filename):\n",
        "    \n",
        "    # Loading the file containing the list of photo identifier\n",
        "    file = load_file(filename)\n",
        "    \n",
        "    # Creating a list for storing the identifiers\n",
        "    photos = list()\n",
        "    \n",
        "    # Traversing the file one line at a time\n",
        "    for line in file.split('\\n'):\n",
        "        if len(line) < 1:\n",
        "            continue\n",
        "        \n",
        "        # Image name contains the extension as well but we need just the name\n",
        "        identifier = line.split('.')[0]\n",
        "        \n",
        "        # Adding it to the list of photos\n",
        "        photos.append(identifier)\n",
        "        \n",
        "    # Returning the set of photos created\n",
        "    return set(photos)\n",
        "\n",
        "\n",
        "# loading the cleaned descriptions that we created earlier\n",
        "# we will only be loading the descriptions of the images that we will use for training\n",
        "# hence we need to pass the set of train photos that the above function will be returning\n",
        "\n",
        "def load_clean_descriptions(filename, photos):\n",
        "    \n",
        "    #loading the cleaned description file\n",
        "    file = load_file(filename)\n",
        "    \n",
        "    #creating a dictionary of descripitions for storing the photo to description mapping of train images\n",
        "    descriptions = dict()\n",
        "    \n",
        "    #traversing the file line by line\n",
        "    for line in file.split('\\n'):\n",
        "        # splitting the line at white spaces\n",
        "        words = line.split()\n",
        "        \n",
        "        # the first word will be the image name and the rest will be the description of that particular image\n",
        "        image_id, image_description = words[0], words[1:]\n",
        "        \n",
        "        # we want to load only those description which corresponds to the set of photos we provided as argument\n",
        "        if image_id in photos:\n",
        "            #creating list of description if needed\n",
        "            if image_id not in descriptions:\n",
        "                descriptions[image_id] = list()\n",
        "            \n",
        "            #the model we will develop will generate a caption given a photo, \n",
        "            #and the caption will be generated one word at a time. \n",
        "            #The sequence of previously generated words will be provided as input. \n",
        "            #Therefore, we will need a ‘first word’ to kick-off the generation process \n",
        "            #and a ‘last word‘ to signal the end of the caption.\n",
        "            #we will use 'startseq' and 'endseq' for this purpose\n",
        "            #also we have to convert image description back to string\n",
        "            \n",
        "            desc = 'startseq ' + ' '.join(image_description) + ' endseq'\n",
        "            descriptions[image_id].append(desc)\n",
        "            \n",
        "    return descriptions\n",
        "\n",
        "# function to load the photo features created using the VGG16 model\n",
        "def load_photo_features(filename, photos):\n",
        "    \n",
        "    #this will load the entire features\n",
        "    all_features = load(open(filename, 'rb'))\n",
        "    \n",
        "    #we are interested in loading the features of the required photos only\n",
        "    features = {k: all_features[k] for k in photos}\n",
        "    \n",
        "    return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mm0_iUaLFln3",
        "outputId": "c64141b3-a26e-4bc0-90ff-5ffe43904c83"
      },
      "source": [
        "filename = 'Flickr_8k.trainImages.txt'\n",
        "\n",
        "train = load_photo_identifiers(filename)\n",
        "print('Dataset: ',len(train))\n",
        "\n",
        "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
        "print('Descriptions: train=', len(train_descriptions))\n",
        "\n",
        "train_features = load_photo_features('features.pkl', train)\n",
        "print('Photos: train=', len(train_features))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset:  6000\n",
            "Descriptions: train= 6000\n",
            "Photos: train= 6000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pm70-G_gFqc5"
      },
      "source": [
        "# convert a dictionary of clean descriptions to a list of descriptions\n",
        "def to_lines(descriptions):\n",
        "    all_desc = list()\n",
        "    for key in descriptions.keys():\n",
        "        [all_desc.append(d) for d in descriptions[key]]\n",
        "    return all_desc\n",
        "\n",
        "# Given the descriptions, fit a tokenizer\n",
        "\n",
        "# TOKENIZER CLASS:\n",
        "# Turns each text into either a sequence of integers \n",
        "# (each integer being the index of a token in a dictionary) \n",
        "# or, into a vector where the coefficient for each token could be binary, based on word count, based on tf-idf...\n",
        "\n",
        "def create_tokenizer(descriptions):\n",
        "    lines = to_lines(descriptions)\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDbFMA-2Fsof",
        "outputId": "bbaab656-00fd-4638-917d-65fbad58c211"
      },
      "source": [
        "tokenizer = create_tokenizer(train_descriptions)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary Size: ', vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary Size:  7579\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHFR6io7FuU1"
      },
      "source": [
        "#calculated the length of description with most words\n",
        "def max_lengthTEMP(descriptions):\n",
        "    lines = to_lines(descriptions)\n",
        "    return max(len(d.split()) for d in lines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LUUdUcaFyqp"
      },
      "source": [
        "#the below function loop forever with a while loop and within this, \n",
        "#loop over each image in the image directory. \n",
        "#For each image filename, we can load the image and \n",
        "#create all of the input-output sequence pairs from the image’s description.\n",
        "\n",
        "#data generator, intended to be used in a call to model.fit_generator()\n",
        "def data_generator(descriptions, photos, tokenizer, max_length):\n",
        "    while 1:\n",
        "        for key, description_list in descriptions.items():\n",
        "            #retrieve photo features\n",
        "            photo = photos[key][0]\n",
        "            input_image, input_sequence, output_word = create_sequences(tokenizer, max_length, description_list, photo)\n",
        "            yield [(input_image, input_sequence), output_word]\n",
        "\n",
        "            \n",
        "#we are calling the create_sequence() function to create \n",
        "#a batch worth of data for a single photo rather than an entire dataset. \n",
        "#This means that we must update the create_sequences() function \n",
        "#to delete the “iterate over all descriptions” for-loop.            \n",
        "#Updated create sequence function for data_generator\n",
        "def create_sequences(tokenizer, max_length, desc_list, photo):\n",
        "    X1, X2, y = list(), list(), list()\n",
        "    # walk through each description for the image\n",
        "    for desc in desc_list:\n",
        "        # encode the sequence\n",
        "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
        "        # split one sequence into multiple X,y pairs\n",
        "        for i in range(1, len(seq)):\n",
        "            # split into input and output pair\n",
        "            in_seq, out_seq = seq[:i], seq[i]\n",
        "            # pad input sequence\n",
        "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "            # encode output sequence\n",
        "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "            # store\n",
        "            X1.append(photo)\n",
        "            X2.append(in_seq)\n",
        "            y.append(out_seq)\n",
        "    return array(X1), array(X2), array(y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwZ-6yrDF1jN"
      },
      "source": [
        "from keras.utils import plot_model\n",
        "# define the captioning model\n",
        "def define_model(vocab_size, max_length):\n",
        "    \n",
        "    # feature extractor model\n",
        "    inputs1 = Input(shape=(4096,))\n",
        "    #Dropout is used to eliminate nodes that might create overfitting\n",
        "    fe1 = Dropout(0.5)(inputs1)\n",
        "    #256 is number of nodes in second layer of VGG16\n",
        "    fe2 = Dense(256, activation='relu')(fe1)\n",
        "\n",
        "    # sequence model\n",
        "    inputs2 = Input(shape=(max_length,))\n",
        "    #Using embedding layer to compress input feature space\n",
        "    se1 = Embedding(vocab_size, 256)(inputs2)\n",
        "    se2 = Dropout(0.5)(se1)\n",
        "    se3 = LSTM(256)(se2)\n",
        "\n",
        "    # decoder model\n",
        "    decoder1 = add((fe2, se3))\n",
        "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "    \n",
        "    # tie it together [image, seq] [word]\n",
        "    model = Model(inputs=(inputs1, inputs2), outputs=outputs)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "    \n",
        "    # summarize model\n",
        "    print(model.summary())\n",
        "    plot_model(model, to_file='model.png', show_shapes=True)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ees8PfFAEgCG"
      },
      "source": [
        "filename = 'Flickr_8k.trainImages.txt'\n",
        "train = load_photo_identifiers(filename)\n",
        "print('Dataset: ', len(train))\n",
        "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
        "print('Descriptions: train=', len(train_descriptions))\n",
        "train_features = load_photo_features('features.pkl', train)\n",
        "print('Photos: train=', len(train_features))\n",
        "tokenizer = create_tokenizer(train_descriptions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary Size:', vocab_size)\n",
        "max_length = max_lengthTEMP(train_descriptions)\n",
        "print('Description Length: ', max_length)\n",
        "\n",
        "model = define_model(vocab_size, max_length)\n",
        "epochs = 20\n",
        "steps = len(train_descriptions)\n",
        "for i in range(epochs):\n",
        "    generator = data_generator(train_descriptions, train_features, tokenizer, max_length)\n",
        "    model.save('model_' + str(i) + '.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGT6ysLTF9En"
      },
      "source": [
        "#this function maps an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == integer:\n",
        "            return word\n",
        "    return None\n",
        "\n",
        "#The function below generates a textual description given a trained model, \n",
        "#and a given prepared photo as input. It calls the function word_for_id() \n",
        "#in order to map an integer prediction back to a word.\n",
        "def generate_desc(model, tokenizer, photo, max_length):\n",
        "    #start tge generation process\n",
        "    in_text = 'startseq'\n",
        "    #iterating over the max_length since the maximum length of the description can be that only\n",
        "    for i in range(max_length):\n",
        "        #integer ncoding input sequence\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        #padding the input\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "        #predicting next word\n",
        "        #the predict function will return probability\n",
        "        prob = model.predict([photo,sequence], verbose=0)\n",
        "        #converting the probability to integer\n",
        "        prob = argmax(prob)\n",
        "        #calling the word_for_id function in order to map integer to word\n",
        "        word = word_for_id(prob, tokenizer)\n",
        "        #breaking if word cannot be mapped\n",
        "        if word is None:\n",
        "            break\n",
        "        #appending as input\n",
        "        in_text += ' ' + word\n",
        "        #break if end is predicted\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "    return in_text\n",
        "\n",
        "#the below function evaluates the skill of the model\n",
        "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
        "    actual, predicted = list(), list()\n",
        "    for key, desc_list in descriptions.items():\n",
        "        prediction = generate_desc(model, tokenizer, photos[key], max_length)\n",
        "        actual_desc = [d.split() for d in desc_list]\n",
        "        actual.append(actual_desc)\n",
        "        predicted.append(prediction.split())\n",
        "\n",
        "    print('BLEU-1: ', corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "    print('BLEU-2: ', corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "    print('BLEU-3: ', corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "    print('BLEU-4: ', corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "    \n",
        "def max_length(descriptions):\n",
        "    lines = to_lines(descriptions)\n",
        "    return max(len(d.split()) for d in lines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NKNq0SLoCAx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c0c1082-a4ab-49d6-ddaa-7848b2d51270"
      },
      "source": [
        "filename = 'Flickr_8k.trainImages.txt'\n",
        "train = load_photo_identifiers(filename)\n",
        "print('Dataset: ', len(train))\n",
        "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
        "print('Descriptions: train=', len(train_descriptions))\n",
        "tokenizer = create_tokenizer(train_descriptions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary Size: ', vocab_size)\n",
        "max_length = max_lengthTEMP(train_descriptions)\n",
        "print('Description Length: ,', max_length)\n",
        "\n",
        "filename = 'Flickr_8k.testImages.txt'\n",
        "test = load_photo_identifiers(filename)\n",
        "print('Dataset: ', len(test))\n",
        "test_descriptions = load_clean_descriptions('descriptions.txt', test)\n",
        "print('Descriptions: test=', len(test_descriptions))\n",
        "test_features = load_photo_features('features.pkl', test)\n",
        "print('Photos: test=', len(test_features))\n",
        "\n",
        "#filename = 'model_12.h5'\n",
        "#evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset:  6000\n",
            "Descriptions: train= 6000\n",
            "Vocabulary Size:  7579\n",
            "Description Length: , 34\n",
            "Dataset:  1000\n",
            "Descriptions: test= 1000\n",
            "Photos: test= 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6GSyLnYGGvQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10716cbc-044a-4f68-945c-5eb727809e4a"
      },
      "source": [
        "filename = 'Flickr_8k.trainImages.txt'\n",
        "train = load_photo_identifiers(filename)\n",
        "print('Dataset: ', len(train))\n",
        "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
        "print('Descriptions: train=', len(train_descriptions))\n",
        "tokenizer = create_tokenizer(train_descriptions)\n",
        "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset:  6000\n",
            "Descriptions: train= 6000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENs8_SrlEEMV"
      },
      "source": [
        "def extract_features(filename):\n",
        "    model = VGG16()\n",
        "    model.layers.pop()\n",
        "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
        "    image = load_img(filename, target_size=(224, 224))\n",
        "    image = img_to_array(image)\n",
        "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "    image = preprocess_input(image)\n",
        "    feature = model.predict(image, verbose=0)\n",
        "    return feature\n",
        "\n",
        "def word_for_id(integer, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == integer:\n",
        "            return word\n",
        "    return None\n",
        "\n",
        "def generate_desc(model, tokenizer, photo, max_length):\n",
        "    in_text = 'startseq'\n",
        "    for i in range(max_length):\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "        yhat = model.predict([photo,sequence], verbose=0)\n",
        "        yhat = argmax(yhat)\n",
        "        word = word_for_id(yhat, tokenizer)\n",
        "        if word is None:\n",
        "            break\n",
        "        in_text += ' ' + word\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "    return in_text\n",
        "\n",
        "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
        "max_length = 34"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YifhWsCkGNIH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1cd6c64-1e28-4ca9-a9f0-185eb07966eb"
      },
      "source": [
        "path = 'drive/MyDrive/Flicker8k_Dataset/111537222_07e56d5a30.jpg'\n",
        "photo = extract_features(path)\n",
        "\n",
        "description = generate_desc(model, tokenizer, photo, max_length)\n",
        "\n",
        "print(description)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "startseq man in red shirt is standing on rock endseq\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}